{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec78a642-a034-4450-a800-f97e2675f237",
   "metadata": {},
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed31c8-1a9a-4a98-9eeb-ec555f4cbd1b",
   "metadata": {},
   "source": [
    "- This notebook is a beginner's guide to all the steps involved in text-preprocessing.  \n",
    "- All steps involved in converting unstructured data to structured.  \n",
    "- Source of tutorial: [FreeCodeCamp](https://www.freecodecamp.org/news/natural-language-processing-techniques-for-beginners/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8866d78-6ce0-47c4-99cc-1aba948ce4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603568ff-ed0f-4812-a2d9-0521c6389f33",
   "metadata": {},
   "source": [
    "### Pre-processing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf9d94-036d-4b40-bf7b-3c113ced61f3",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Step</th>\n",
    "            <th>Explanation</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Tokenization</td>\n",
    "            <td>convert entire text-data to words or sentence tokens</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Normalization</td>\n",
    "            <td>convert to lowercase, remove numbers, punctuations, accents, special chars (using unicode) etc.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Stemming</td>\n",
    "            <td>convert words to base form. less efficient as compared to lemmatization</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Lemmatization</td>\n",
    "            <td>convert words to base form. more advanced than stemming</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Remove stop-words</td>\n",
    "            <td>Words like \"the\", \"an\" etc. which do not contribute to overall meaning</td>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2b6f0-94f0-4017-8772-c85755356b0d",
   "metadata": {},
   "source": [
    "### 1. Tokenization  \n",
    "\n",
    "i) **Word Tokenization-** each word is a token. the sentences are broken down into single words.  \n",
    "ii) **Sentence Tokenization-** phrases form one token. sentences are broken down into words and phrases.\n",
    "\n",
    "#### Some tokenization algorithms\n",
    "1. **Whitespace tokenization-** create tokens using whitespaces as separator.\n",
    "2. **Pattern tokenization (regex)-** create tokens using provided regex as separator.\n",
    "3. **Punctuation-based tokenization** etc.\n",
    "\n",
    "Type of algorithm used depends on the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b833bf2-2009-4d0e-9dfc-59b1e678ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'For the powerful, crimes are those that others commit.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3a17d0-299c-429b-9355-7d586b674709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'the', 'powerful,', 'crimes', 'are', 'those', 'that', 'others', 'commit.']\n"
     ]
    }
   ],
   "source": [
    "#using python's in-built split() function\n",
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8aa03-9fee-441f-a51f-4430fdbaf9f4",
   "metadata": {},
   "source": [
    "### 2. Normalization\n",
    "\n",
    "Make text-data consistent.\n",
    "\n",
    "- convert all to lowercase.\n",
    "- remove numbers & punctuation marks.\n",
    "- remove accents using Unicodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae616d4f-679e-49ae-b697-9128cd0ef9a7",
   "metadata": {},
   "source": [
    "#### a. Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862aabf5-fbad-4fc8-b8a1-b73218d5beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the moon and back.\n"
     ]
    }
   ],
   "source": [
    "text2 = 'To THe mooN and bAcK.'\n",
    "text2_lower = text2.lower()\n",
    "print(text2_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a2aef-0ce4-415d-9c5c-dbd7c8f242cb",
   "metadata": {},
   "source": [
    "#### b. Remove Punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4397812e-c7a7-4db2-ada2-ea156625aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shefali This is a cool day! as of now\n"
     ]
    }
   ],
   "source": [
    "#all the punctuation marks\n",
    "\n",
    "#re.compile() converts the given patterns to a regex object\n",
    "punctuation_marks = re.compile(r'[{};():,.\"/<>-]')\n",
    "text3 = 'shefali: (This is a cool day!), <as of now>,'\n",
    "text3_clean = punctuation_marks.sub('', text3)\n",
    "print(text3_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284b7d7-cece-4916-8415-a940339e6a7a",
   "metadata": {},
   "source": [
    "#### c. Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb6d4633-db22-4551-befd-2a7bb6ca90b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After the cafe closed, Rene decided to visit his mere.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4 = 'After the café closed, René decided to visit his mère.'\n",
    "accents_regex = re.compile(u\"[\\u0300-\\u036F]|é|è\")\n",
    "accents_regex.sub('e', text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85772a01-2773-4e30-b886-ede774c27f67",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "\n",
    "Reduce the words to their root form.  \n",
    "For e.g.- ***\"study, studying, studied\"*** all get converted to ***\"study\".***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ccf2b4-6cea-4747-a49a-006b4f52ca6e",
   "metadata": {},
   "source": [
    "### 4. Lemmatisation\n",
    "\n",
    "Better than stemming.  \n",
    "It takes into consideration the \"part of speech\" whether word is mentioned as a noun or verb.  \n",
    "Before converting a word to base form, it takes into account structure of the word.  \n",
    "It is more computationally expensive as it is more accurate than stemming.  \n",
    "<hr>\n",
    "\n",
    "For e.g. ***\"I drink water every day\"*** & ***\"I like that drink.\"***  \n",
    "The word drink is a verb in first sentence & a noun in the other.  \n",
    "Lemmetisation keeps these differences in check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6638e6-cc9d-4da8-a01e-f3b9626f1699",
   "metadata": {},
   "source": [
    "### 5. Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57c37d-f125-42ba-a0bc-520e7118dd3a",
   "metadata": {},
   "source": [
    "Stop words includes \"the\", \"an\", \"that\", \"to\", \"because\", \"as\" etc.  \n",
    "Words that do not add any meaning to a sentence.  \n",
    "They should also be removed as they reduce the size of dataset and help improve efficiency of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2d6b3-ecd4-43c3-aa2a-d7c7706ce725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
